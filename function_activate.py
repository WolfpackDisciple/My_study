""" Выбор функции активации для нейрона"""

""" 
В пределах скрытого слоя они как правило одни и теже f(x) = x, а для выходного слоя, как правило
функции активации несколько другие.

    Какие же функции активации следует выбирать при построении нейронных сетей для решение 
    различных задач?




"""


# Пороговая функция активации

# f(x) = ([1, if x>0,5] [0, if x<0,5])
# Относительно редко испоьзуется на практике из-за проблем с производными

# Гиперболический тангенс

# tanh = f(x) = ( e**x - e**-x ) / (e**x + e**-x)

# Логистическая функция

#logf = f(x) = 1/(1+e**-x)
# f`(x) = f(x) * (1 - f(x)) - производная лог функции
""" Гиперболический тангенс и логичтическая функция применяются там где небольшое число 
скрытых слоев... Не более 7 слоев. Если слоев 7 и более, то лучше пользоваться другими акт функциями.

Рекомендуется не применять их в многослойных нейросетях проблемами с нахождением локального градиента..
Так как локальный градиент вычисляется путем произведения функции активации... Но произвадная 
Гиперболического тангенса и Логистической функции меньше единицы и когда производим произведение двух
чисел меньше единицы то у нас получается спадающая экспонента при увеличении числа слоев...

С увеличением слоев  у нас начинает убучаюся только последние слои, в то время как первые слои
нейроннов остаются без обучения ...
"""


""" Чтобы решить эту проблему нужно выбрать функцию производная которой равна единице ..."""
# f(x) = x
# ReLu = f(x) = max(x, 0)  # Релу имеет ограничения от 0 до 1
# В DeepLearning  эта функция становиться незаменимой в качестве активационной функции
# При увеличении слоев используют  RelU и ее модификации



"""  При  испооьзовании скрытых слоев TanH Sigm if <7 layers : else ReLu и ее вариации

Но это только для скрытых слоев, часто для вызодногос слоя функции активации меняются
и становятся другими

Например в задачах регрессии, т.е Б в задачах в которых нужно получить  некое числовое значение...
тогда функция активации выбирается линейной и называется linear

"""


# linear =  f(x) = x # Задачи регрессии это например прогнозирование цены
# определение роста или фильтрация сигналов.. Там где мы имеем дело с каким то выходным 
# числовым значением и с этим числом ничего больше делать не нужно



"""
SOFTMAX
В классификации хорошо если мы можем выходные значения интерпретировать в терминах
вероятности... Отнести предьявляемый вектор к тому или иному классу с определенной вероятностью
y1 = 0,923
y2 = 0, 027  ---> вероятность отношения (_их сумма равна единицу)
y3 = 0, 050

"""
# Для задач классификации:
# В таких задачах функция активации скрытых слоев будет отличаться от функции активации
# выхоного слоя
# SOFTMAX - некое приблежение ./ способ интерпретации в терминах вероятности
# Выход напервом нейроне определятся отношением Экспонент 
# y1 = e**v1 / (e**v1 + e**v2 + e**v3) ; y2 = e**v2 / (e**v1 + e**v2 + e**v3); y3 = e**v3/(e**v1+e**v2+e**v3)
# Решение этих формул приведет к тому что на выходе будут получаться величины в диапазоне от 0 до 1
# а в сумме  они будут давать едиицу;


""" При выборе softmax критерием качется является ---перекрестная энтропия---

"""

# Для корректной реализации обучения нужно выбрать подходящий 
# критерий качества
# Критерии качества 

""" В кдассическом алгорите back propagation критерием качества является минимум средних 
квадратов ошибок
E = 1/2 SIGMA(_N_ : i=1) (d1 - y1)**2 - но этот критерий подходит не для всех задач

Задачи: Распознование, ОБработка текста, ЗАдачи регрессии

Распознование:
    хиндж(hinge)
    бинарная крос-энтропия( binary crossentropy) более двух классов
    категориальная кросс-энтропия (categorical crossentropy) >2 classes


Обработка текста:
    логарифмический гиперболичекий косинус (logcsh);


Задачи регресии:
    средний квадрат ощибое (mean squared error)
    средний модуль ошибок (mean absolut error)
    средний абсолютный процент ошибок (mean absolute precentage error) - хороша в прогнозировании
    средний квадрат логарифмических ошибок (mean squared logarithmic error)

"""









